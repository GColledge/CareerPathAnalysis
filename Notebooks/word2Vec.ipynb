{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.special import expit as sigmoid\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory\n",
    "The cell below creates a directory for the tensorboard files to go into as well as where the weights and title2indx dictionary will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\");\n",
    "root_logdir = \"tensorFlow_logs\";\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data function\n",
    "The cell below defines how the data is read in and sets up the input for the rest of the model.\n",
    "\n",
    "**Adjustable Parameters**:\n",
    "* V - the number of elements or dimensions used in the word embedding. I found that 2000 gave decent results but that tensorboard's PCA calculations couldn't handle that big of embeddings.\n",
    "* fileLoc - The location of the input file. **This must be changed for your set up!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    s = s.replace(',', '')\n",
    "    s = s.replace('.', '')\n",
    "    s = s.replace('\\'', '')\n",
    "    s = s.replace('\\\"', '')\n",
    "    return s\n",
    "\n",
    "def get_data():\n",
    "    V = 1000   #The max number of unique job titles. \n",
    "    \n",
    "    fileLoc = \"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/allPaths.txt\"\n",
    "    file = open(fileLoc)\n",
    "    all_titles_counts = {} #dictionary of job titles and thier counts\n",
    "    \n",
    "    # Count the total number of titles\n",
    "    for line in file:\n",
    "        path = remove_punctuation(line).split()\n",
    "        if len(path) > 1:\n",
    "            for title in path:\n",
    "                if title not in all_titles_counts:\n",
    "                    all_titles_counts[title] = 0\n",
    "                all_titles_counts[title] += 1\n",
    "    \n",
    "    #if there are fewer titles than V, reassign it.\n",
    "    V = min(len(all_titles_counts), V)\n",
    "    # sort the dictionary by the number of times a title appears.\n",
    "    all_titles_counts = sorted(all_titles_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #get the top V number of titles but reserve one for the \"unknowns\" which\n",
    "    # is really just for anything that didn't show up enough to make the list.\n",
    "    top_titles = [t for t, count in all_titles_counts[:V-1]] + ['<UNK>']\n",
    "    #create a mapping from title to index for top_titles.\n",
    "    title2indx = {t:i for i, t in enumerate(top_titles)}\n",
    "    unk = title2indx['<UNK>']\n",
    "    print(\"Index for the 'unknowns' is : \" + str(unk))\n",
    "    \n",
    "    paths = [] #This will be a list of vectorized job paths.\n",
    "    for line in open(fileLoc):\n",
    "        path = remove_punctuation(line).split() # 'path' is the string of the path\n",
    "        if len(path) > 1:    #only a path with two job titles will give any context\n",
    "            p = [title2indx[t] if t in title2indx else unk for t in path] # 'p' is the vectorized path\n",
    "            paths.append(p)\n",
    "            \n",
    "    print(paths[:3])\n",
    "    print(\"index for 'software_engineer': \" + str(title2indx[\"software_engineer\"]))\n",
    "    print(\"number of paths from get data: \" + str(len(paths)))\n",
    "    return paths, title2indx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling distribution\n",
    "The function below defines the distribution for negative sampling. Negative sampling is used in this code as an approximation to softmax. Negative sampling is reported to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative sampling is a faster way to approximate the softmax function when using skipgram\n",
    "def get_negative_sampling_distribution(paths):\n",
    "    # Pn(t) = prob of title occuring\n",
    "    # we would like to sample the negative samples such that\n",
    "    # titles that occur more often should be sampled more often\n",
    "    \n",
    "    title_freq = {}\n",
    "    title_count = sum(len(path) for path in paths)\n",
    "    for path in paths:\n",
    "        for title in path:\n",
    "            if title not in title_freq:\n",
    "                 title_freq[title] = 0\n",
    "            title_freq[title] += 1\n",
    "        \n",
    "    # vocab size\n",
    "    V = len(title_freq)\n",
    "        \n",
    "    p_neg = np.zeros(V)\n",
    "    for j in range(V):\n",
    "        p_neg[j] = title_freq[j]**0.75 #0.75 is an adjustable parameter.\n",
    "        # This comes form the modified uniform distribution.\n",
    "            \n",
    "    # normalize p_neg\n",
    "    p_neg = p_neg / p_neg.sum()\n",
    "        \n",
    "    assert(np.all(p_neg > 0))\n",
    "    return p_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get context\n",
    "The function below gets the context words for the skipgram model. Note that this function is modified just a bit from the regular skipgram model in that it only takes context words that come after the selected position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, path, context_size):\n",
    "    # input:\n",
    "    # a path of the form: x x x pos c c c x x x\n",
    "    #output:\n",
    "    # the context paths: c c c \n",
    "    \n",
    "#     start = max(0, pos - context_size) #this is if you want backward context as well.\n",
    "    start = pos # This is for forward context only. Like next job. This is something to play around with.\n",
    "    end_ = min(len(path), pos + context_size)\n",
    "    \n",
    "    context = []\n",
    "    for context_pos, context_title_indx in enumerate(path[start:end_], start=start):\n",
    "        if context_pos != pos:\n",
    "            #don't include the input title itself as a target\n",
    "            context.append(context_title_indx)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions\n",
    "The function below gives some indication of how well our model fits. Read it and the analogy function carefully to understand what it is reporting. More calls to analogy can be put into the test_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(title2indx, W, V):\n",
    "\n",
    "    indx2title = {i:t for t, i in title2indx.items()}\n",
    "        # I am taking the title embedding as the average of the two weight vectors\n",
    "    for word_embedding in (W, (W + V.T)/2):\n",
    "        print(\"_______Testing_________\")\n",
    "        analogy('research_intern', 'graduate_research_assistant', 'intern', 'research_assistant', title2indx, indx2title, W)\n",
    "        analogy('engineering_intern', 'engineer', 'software_engineering_intern', 'software_engineer', title2indx, indx2title, W)\n",
    "        analogy('software_engineer', 'software_developer', 'director', 'ceo', title2indx, indx2title, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive1, negative1, positive2, negative2, title2indx, indx2title, W):\n",
    "    V, D = W.shape\n",
    "    \n",
    "    print(\"--------------\\ntesting: %s - %s = %s - %s\" %(positive1, negative1, positive2, negative2))\n",
    "    for t in (positive1, negative1, positive2, negative2):\n",
    "        if t not in (title2indx):\n",
    "            print(t + \" is not in the used titles.\")\n",
    "            \n",
    "    p1 = W[title2indx[positive1]]\n",
    "    p2 = W[title2indx[positive2]]\n",
    "    n1 = W[title2indx[negative1]]\n",
    "    n2 = W[title2indx[negative2]]\n",
    "    \n",
    "    vec = p1 - n1 + n2\n",
    "    \n",
    "    #using the cosine distance rather than the euclidian distance.\n",
    "    distances = pairwise_distances(vec.reshape(1,D), W, metric='cosine').reshape(V)\n",
    "    indx = distances.argsort()[:5] #gets the index of the 5 closest vectors\n",
    "    bestIndx = -1\n",
    "    # remove the input titles as possibilities\n",
    "    keep_out = [title2indx[t] for t in (positive1, negative1, negative2)]\n",
    "    for r in indx:\n",
    "        if r not in keep_out:\n",
    "            bestIndx = r;\n",
    "            break\n",
    "     \n",
    "    for i in indx:\n",
    "        print(indx2title[i], distances[i])\n",
    "        \n",
    "    print(\"Distance to %s: \" % positive2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "The model can take a long time to run, so when running the model, a number of files are saved in order to avoid needing to run the model multiple times. The Load model will read those files and give back the title2indx dictionary as well as the hidden layer weights. I have typically been using (W + V.T)/2 as the official title embedding, as seen in the test_model function. This can be changed based on future testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(directory):\n",
    "    with open('%s/title2indx.json' % directory) as f:\n",
    "        title2indx = json.load(f)\n",
    "    npz = np.load('%s/weights.npz' % directory)\n",
    "    W = npz['arr_0']\n",
    "    V = npz['arr_1']\n",
    "    return title2indx, W, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "__DO NOT RUN THIS CELL WITHOUT READING THIS FIRST!__\n",
    "\n",
    "This is where the magic happens! There are a number of adjustable parameters that can be tweaked as described below. This cell can take a long time to run. This is of course dependant on other parameters as well so make sure you have the parameters set the way you want them.\n",
    "\n",
    "In this cell I am using tensorflow's \"Interactive Session\" which needs to be closed explicitly. The closing statment is the bottom cell. The interactive session can cause some wierd things to happen if it is left open and/or run out of order. Look [here](https://www.tensorflow.org/api_docs/python/tf/InteractiveSession) for details.\n",
    "\n",
    "**Adjustable parameters:**\n",
    "* context_size - The size of the window used to get context words. This is a little different in this code than normal. see the get_context function above.\n",
    "* learning_rate - the starting learning rate\n",
    "* final_learning_rate - the final learning rate. The learning rate will change until it reaches this value.\n",
    "* num_negatives - number of negative samples to use. see [this site](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) for more of a description of negative sampling.\n",
    "* epochs - the number of training rounds to go through. So far, I have found that this has the largest impact on run time. I have found 300 epochs gives decent results.\n",
    "* D - the word embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for the 'unknowns' is : 999\n",
      "[[463, 51, 1, 1, 1], [999, 999, 135, 999], [62, 8, 394, 2, 394, 394]]\n",
      "index for 'software_engineer': 1\n",
      "number of paths from get data: 892370\n",
      "Total number of titles: 5750593\n",
      "epoch complete: 0 cost: 1967.2043406963348 dt: 0:00:07.991034\n",
      "epoch complete: 1 cost: 1642.393219947815 dt: 0:00:07.927381\n",
      "epoch complete: 2 cost: 1400.6339645385742 dt: 0:00:07.885824\n",
      "epoch complete: 3 cost: 1288.9442400932312 dt: 0:00:07.817117\n",
      "epoch complete: 4 cost: 1171.862764120102 dt: 0:00:07.718405\n",
      "epoch complete: 5 cost: 1098.5783232450485 dt: 0:00:07.925106\n",
      "epoch complete: 6 cost: 1046.9535006284714 dt: 0:00:07.971907\n",
      "epoch complete: 7 cost: 959.7426824569702 dt: 0:00:08.004297\n",
      "epoch complete: 8 cost: 946.9459958076477 dt: 0:00:08.203631\n",
      "epoch complete: 9 cost: 885.7803808450699 dt: 0:00:07.991659\n",
      "epoch complete: 10 cost: 844.0623033046722 dt: 0:00:08.123245\n",
      "epoch complete: 11 cost: 823.5379295349121 dt: 0:00:07.873708\n",
      "epoch complete: 12 cost: 808.398792386055 dt: 0:00:08.303836\n",
      "epoch complete: 13 cost: 785.6558372974396 dt: 0:00:08.360564\n",
      "epoch complete: 14 cost: 739.8360209465027 dt: 0:00:07.777698\n",
      "epoch complete: 15 cost: 741.5319402217865 dt: 0:00:08.058375\n",
      "epoch complete: 16 cost: 711.5836656093597 dt: 0:00:08.010239\n",
      "epoch complete: 17 cost: 700.2409977912903 dt: 0:00:08.118705\n",
      "epoch complete: 18 cost: 700.4907137155533 dt: 0:00:08.161781\n",
      "epoch complete: 19 cost: 683.2710955142975 dt: 0:00:08.002130\n",
      "epoch complete: 20 cost: 679.5264812707901 dt: 0:00:08.115348\n",
      "epoch complete: 21 cost: 650.2850948572159 dt: 0:00:08.292872\n",
      "epoch complete: 22 cost: 642.0683588981628 dt: 0:00:07.916942\n",
      "epoch complete: 23 cost: 638.7687082290649 dt: 0:00:08.040709\n",
      "epoch complete: 24 cost: 616.7917976379395 dt: 0:00:08.087065\n",
      "epoch complete: 25 cost: 606.5215308666229 dt: 0:00:07.944588\n",
      "epoch complete: 26 cost: 603.4632379412651 dt: 0:00:07.922557\n",
      "epoch complete: 27 cost: 594.3611193299294 dt: 0:00:07.974104\n",
      "epoch complete: 28 cost: 596.078772187233 dt: 0:00:08.029611\n",
      "epoch complete: 29 cost: 585.0543808937073 dt: 0:00:07.822699\n",
      "epoch complete: 30 cost: 578.717953145504 dt: 0:00:07.968075\n",
      "epoch complete: 31 cost: 564.1203154921532 dt: 0:00:08.019914\n",
      "epoch complete: 32 cost: 560.8927331566811 dt: 0:00:08.090024\n",
      "epoch complete: 33 cost: 554.420584499836 dt: 0:00:07.992568\n",
      "epoch complete: 34 cost: 555.0486164689064 dt: 0:00:07.970302\n",
      "epoch complete: 35 cost: 551.7890369296074 dt: 0:00:08.007681\n",
      "epoch complete: 36 cost: 546.0734204053879 dt: 0:00:08.246224\n",
      "epoch complete: 37 cost: 542.5148058533669 dt: 0:00:08.380660\n",
      "epoch complete: 38 cost: 527.6347327828407 dt: 0:00:08.072110\n",
      "epoch complete: 39 cost: 532.6970770359039 dt: 0:00:08.163970\n",
      "epoch complete: 40 cost: 536.9172007441521 dt: 0:00:08.148755\n",
      "epoch complete: 41 cost: 527.3638505339622 dt: 0:00:08.153606\n",
      "epoch complete: 42 cost: 521.8327256441116 dt: 0:00:08.071986\n",
      "epoch complete: 43 cost: 511.62253963947296 dt: 0:00:08.069259\n",
      "epoch complete: 44 cost: 513.1373739242554 dt: 0:00:07.673175\n",
      "epoch complete: 45 cost: 513.6404861211777 dt: 0:00:07.868164\n",
      "epoch complete: 46 cost: 503.37639033794403 dt: 0:00:07.829245\n",
      "epoch complete: 47 cost: 505.7897826433182 dt: 0:00:07.968455\n",
      "epoch complete: 48 cost: 498.1216816306114 dt: 0:00:08.196778\n",
      "epoch complete: 49 cost: 495.10208970308304 dt: 0:00:09.128678\n",
      "epoch complete: 50 cost: 496.97698163986206 dt: 0:00:08.322550\n",
      "epoch complete: 51 cost: 498.2561579346657 dt: 0:00:08.373703\n",
      "epoch complete: 52 cost: 493.64251136779785 dt: 0:00:08.386913\n",
      "epoch complete: 53 cost: 498.45145535469055 dt: 0:00:08.040446\n",
      "epoch complete: 54 cost: 486.52344143390656 dt: 0:00:08.635474\n",
      "epoch complete: 55 cost: 489.325148999691 dt: 0:00:08.848812\n",
      "epoch complete: 56 cost: 492.0687301158905 dt: 0:00:08.665489\n",
      "epoch complete: 57 cost: 483.88305765390396 dt: 0:00:08.339000\n",
      "epoch complete: 58 cost: 478.3724561929703 dt: 0:00:07.937358\n",
      "epoch complete: 59 cost: 476.88922476768494 dt: 0:00:08.069178\n",
      "epoch complete: 60 cost: 479.4529095888138 dt: 0:00:08.309648\n",
      "epoch complete: 61 cost: 464.95651161670685 dt: 0:00:08.042518\n",
      "epoch complete: 62 cost: 471.4907760620117 dt: 0:00:08.453104\n",
      "epoch complete: 63 cost: 474.1607164144516 dt: 0:00:08.472148\n",
      "epoch complete: 64 cost: 469.27514946460724 dt: 0:00:08.013981\n",
      "epoch complete: 65 cost: 472.8734805583954 dt: 0:00:07.965059\n",
      "epoch complete: 66 cost: 461.3798859119415 dt: 0:00:08.160349\n",
      "epoch complete: 67 cost: 462.93649846315384 dt: 0:00:08.156364\n",
      "epoch complete: 68 cost: 458.16912508010864 dt: 0:00:08.222196\n",
      "epoch complete: 69 cost: 463.7886034846306 dt: 0:00:08.145027\n",
      "epoch complete: 70 cost: 458.95799309015274 dt: 0:00:08.273896\n",
      "epoch complete: 71 cost: 453.23684400320053 dt: 0:00:07.858343\n",
      "epoch complete: 72 cost: 460.11588352918625 dt: 0:00:07.865127\n",
      "epoch complete: 73 cost: 453.31155490875244 dt: 0:00:08.370844\n",
      "epoch complete: 74 cost: 466.6757596731186 dt: 0:00:07.888218\n",
      "epoch complete: 75 cost: 452.39805430173874 dt: 0:00:07.865330\n",
      "epoch complete: 76 cost: 454.4323425292969 dt: 0:00:07.943797\n",
      "epoch complete: 77 cost: 446.7970095872879 dt: 0:00:08.069644\n",
      "epoch complete: 78 cost: 461.7716947197914 dt: 0:00:08.133512\n",
      "epoch complete: 79 cost: 453.5337921977043 dt: 0:00:08.047404\n",
      "epoch complete: 80 cost: 454.30926674604416 dt: 0:00:07.955797\n",
      "epoch complete: 81 cost: 446.89341974258423 dt: 0:00:07.985432\n",
      "epoch complete: 82 cost: 443.7587018609047 dt: 0:00:08.017886\n",
      "epoch complete: 83 cost: 449.1826332807541 dt: 0:00:08.003854\n",
      "epoch complete: 84 cost: 446.4138432741165 dt: 0:00:08.494224\n",
      "epoch complete: 85 cost: 443.15715342760086 dt: 0:00:08.614600\n",
      "epoch complete: 86 cost: 442.27921962738037 dt: 0:00:08.033529\n",
      "epoch complete: 87 cost: 441.13292849063873 dt: 0:00:07.951923\n",
      "epoch complete: 88 cost: 436.9505195617676 dt: 0:00:08.186128\n",
      "epoch complete: 89 cost: 448.20391350984573 dt: 0:00:08.007300\n",
      "epoch complete: 90 cost: 444.5004974603653 dt: 0:00:08.004424\n",
      "epoch complete: 91 cost: 447.83352440595627 dt: 0:00:08.263650\n",
      "epoch complete: 92 cost: 442.4394056200981 dt: 0:00:07.659622\n",
      "epoch complete: 93 cost: 437.1916037797928 dt: 0:00:07.736968\n",
      "epoch complete: 94 cost: 434.65441596508026 dt: 0:00:07.917818\n",
      "epoch complete: 95 cost: 433.46310859918594 dt: 0:00:08.444678\n",
      "epoch complete: 96 cost: 433.311594247818 dt: 0:00:07.820482\n",
      "epoch complete: 97 cost: 435.3384709954262 dt: 0:00:07.797853\n",
      "epoch complete: 98 cost: 433.70613718032837 dt: 0:00:08.015172\n",
      "epoch complete: 99 cost: 436.4175676703453 dt: 0:00:07.794223\n",
      "epoch complete: 100 cost: 439.8328980207443 dt: 0:00:07.917777\n",
      "epoch complete: 101 cost: 431.61705672740936 dt: 0:00:07.923437\n",
      "epoch complete: 102 cost: 438.604744553566 dt: 0:00:08.084393\n",
      "epoch complete: 103 cost: 440.4929342865944 dt: 0:00:08.008340\n",
      "epoch complete: 104 cost: 435.2158635854721 dt: 0:00:08.354397\n",
      "epoch complete: 105 cost: 434.8576921224594 dt: 0:00:08.006320\n",
      "epoch complete: 106 cost: 435.3101727962494 dt: 0:00:08.149141\n",
      "epoch complete: 107 cost: 431.1166945695877 dt: 0:00:08.386650\n",
      "epoch complete: 108 cost: 439.7368780374527 dt: 0:00:08.064647\n",
      "epoch complete: 109 cost: 435.79322254657745 dt: 0:00:08.499343\n",
      "epoch complete: 110 cost: 431.7903319001198 dt: 0:00:07.923546\n",
      "epoch complete: 111 cost: 427.9698443412781 dt: 0:00:07.858216\n",
      "epoch complete: 112 cost: 429.63437616825104 dt: 0:00:07.966232\n",
      "epoch complete: 113 cost: 425.8289512395859 dt: 0:00:08.091136\n",
      "epoch complete: 114 cost: 436.37570375204086 dt: 0:00:07.972530\n",
      "epoch complete: 115 cost: 434.6420584321022 dt: 0:00:08.237688\n",
      "epoch complete: 116 cost: 425.5116477608681 dt: 0:00:08.109487\n",
      "epoch complete: 117 cost: 427.53578329086304 dt: 0:00:08.000646\n",
      "epoch complete: 118 cost: 423.8872236609459 dt: 0:00:07.981487\n",
      "epoch complete: 119 cost: 434.2225115299225 dt: 0:00:08.066140\n",
      "epoch complete: 120 cost: 427.1457903981209 dt: 0:00:07.935744\n",
      "epoch complete: 121 cost: 422.2535329461098 dt: 0:00:07.991460\n",
      "epoch complete: 122 cost: 431.8335974216461 dt: 0:00:07.875286\n",
      "epoch complete: 123 cost: 426.14242058992386 dt: 0:00:08.088226\n",
      "epoch complete: 124 cost: 430.6688907146454 dt: 0:00:07.987002\n",
      "epoch complete: 125 cost: 421.2460131049156 dt: 0:00:08.223449\n",
      "epoch complete: 126 cost: 426.36784636974335 dt: 0:00:07.974745\n",
      "epoch complete: 127 cost: 426.20640283823013 dt: 0:00:08.016343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch complete: 128 cost: 423.20723980665207 dt: 0:00:08.328235\n",
      "epoch complete: 129 cost: 425.40065491199493 dt: 0:00:08.093781\n",
      "epoch complete: 130 cost: 431.30132526159286 dt: 0:00:08.016805\n",
      "epoch complete: 131 cost: 425.6585069298744 dt: 0:00:08.087742\n",
      "epoch complete: 132 cost: 424.182391166687 dt: 0:00:08.289590\n",
      "epoch complete: 133 cost: 419.48561292886734 dt: 0:00:08.067029\n",
      "epoch complete: 134 cost: 425.4871554374695 dt: 0:00:07.912004\n",
      "epoch complete: 135 cost: 424.6525406241417 dt: 0:00:08.195228\n",
      "epoch complete: 136 cost: 426.7646389603615 dt: 0:00:08.205188\n",
      "epoch complete: 137 cost: 424.9267703294754 dt: 0:00:08.694032\n",
      "epoch complete: 138 cost: 421.9977750778198 dt: 0:00:08.588743\n",
      "epoch complete: 139 cost: 420.8027784228325 dt: 0:00:08.115401\n",
      "epoch complete: 140 cost: 423.09764724969864 dt: 0:00:08.214258\n",
      "epoch complete: 141 cost: 424.00392895936966 dt: 0:00:08.174151\n",
      "epoch complete: 142 cost: 425.20516431331635 dt: 0:00:08.171138\n",
      "epoch complete: 143 cost: 422.88198894262314 dt: 0:00:07.963364\n",
      "epoch complete: 144 cost: 421.7690210342407 dt: 0:00:07.928479\n",
      "epoch complete: 145 cost: 424.18604922294617 dt: 0:00:08.133130\n",
      "epoch complete: 146 cost: 419.1362042427063 dt: 0:00:07.952759\n",
      "epoch complete: 147 cost: 411.0071145296097 dt: 0:00:07.915301\n",
      "epoch complete: 148 cost: 411.48416697978973 dt: 0:00:08.076001\n",
      "epoch complete: 149 cost: 417.95853650569916 dt: 0:00:07.892755\n",
      "epoch complete: 150 cost: 417.1124954223633 dt: 0:00:08.358519\n",
      "epoch complete: 151 cost: 427.3138324022293 dt: 0:00:07.850129\n",
      "epoch complete: 152 cost: 422.15411698818207 dt: 0:00:07.893194\n",
      "epoch complete: 153 cost: 412.4912303686142 dt: 0:00:08.004370\n",
      "epoch complete: 154 cost: 424.78087198734283 dt: 0:00:07.852557\n",
      "epoch complete: 155 cost: 424.9393861889839 dt: 0:00:07.864172\n",
      "epoch complete: 156 cost: 409.33974343538284 dt: 0:00:07.811425\n",
      "epoch complete: 157 cost: 420.70327603816986 dt: 0:00:08.015738\n",
      "epoch complete: 158 cost: 418.0392287373543 dt: 0:00:07.957468\n",
      "epoch complete: 159 cost: 423.5708855986595 dt: 0:00:07.990548\n",
      "epoch complete: 160 cost: 422.14928781986237 dt: 0:00:08.082375\n",
      "epoch complete: 161 cost: 423.2934031486511 dt: 0:00:08.006394\n",
      "epoch complete: 162 cost: 417.87799602746964 dt: 0:00:07.938232\n",
      "epoch complete: 163 cost: 409.4161179661751 dt: 0:00:08.486160\n",
      "epoch complete: 164 cost: 416.5788064599037 dt: 0:00:08.414795\n",
      "epoch complete: 165 cost: 422.3810498714447 dt: 0:00:08.139543\n",
      "epoch complete: 166 cost: 416.99550026655197 dt: 0:00:07.856311\n",
      "epoch complete: 167 cost: 412.22720843553543 dt: 0:00:07.893214\n",
      "epoch complete: 168 cost: 420.39796710014343 dt: 0:00:08.208583\n",
      "epoch complete: 169 cost: 419.9925100803375 dt: 0:00:08.087020\n",
      "epoch complete: 170 cost: 411.1854105591774 dt: 0:00:08.002426\n",
      "epoch complete: 171 cost: 417.75923585891724 dt: 0:00:08.095745\n",
      "epoch complete: 172 cost: 415.79559832811356 dt: 0:00:08.120895\n",
      "epoch complete: 173 cost: 417.7221109867096 dt: 0:00:07.871788\n",
      "epoch complete: 174 cost: 415.50671672821045 dt: 0:00:07.884790\n",
      "epoch complete: 175 cost: 416.16795617341995 dt: 0:00:08.015464\n",
      "epoch complete: 176 cost: 424.7923016548157 dt: 0:00:08.131168\n",
      "epoch complete: 177 cost: 416.0549266934395 dt: 0:00:08.054376\n",
      "epoch complete: 178 cost: 415.2440281510353 dt: 0:00:08.054311\n",
      "epoch complete: 179 cost: 420.9908075928688 dt: 0:00:08.104928\n",
      "epoch complete: 180 cost: 413.12945556640625 dt: 0:00:08.085010\n",
      "epoch complete: 181 cost: 418.0130311846733 dt: 0:00:07.942437\n",
      "epoch complete: 182 cost: 414.8465330004692 dt: 0:00:07.930844\n",
      "epoch complete: 183 cost: 416.64181047677994 dt: 0:00:08.178551\n",
      "epoch complete: 184 cost: 422.13326317071915 dt: 0:00:08.001445\n",
      "epoch complete: 185 cost: 410.19937109947205 dt: 0:00:07.861371\n",
      "epoch complete: 186 cost: 415.72699052095413 dt: 0:00:08.407398\n",
      "epoch complete: 187 cost: 417.6035552620888 dt: 0:00:08.011447\n",
      "epoch complete: 188 cost: 409.8877393603325 dt: 0:00:08.668202\n",
      "epoch complete: 189 cost: 412.9888679385185 dt: 0:00:08.023551\n",
      "epoch complete: 190 cost: 410.11741346120834 dt: 0:00:07.776371\n",
      "epoch complete: 191 cost: 422.64557260274887 dt: 0:00:08.567113\n",
      "epoch complete: 192 cost: 415.89115685224533 dt: 0:00:08.983700\n",
      "epoch complete: 193 cost: 405.0768492221832 dt: 0:00:08.061882\n",
      "epoch complete: 194 cost: 418.7411075234413 dt: 0:00:07.811458\n",
      "epoch complete: 195 cost: 419.282245695591 dt: 0:00:07.946073\n",
      "epoch complete: 196 cost: 412.96571016311646 dt: 0:00:08.220166\n",
      "epoch complete: 197 cost: 412.5303608775139 dt: 0:00:07.920753\n",
      "epoch complete: 198 cost: 413.3094325661659 dt: 0:00:08.029957\n",
      "epoch complete: 199 cost: 417.3957720398903 dt: 0:00:08.049410\n",
      "epoch complete: 200 cost: 417.5831682085991 dt: 0:00:07.789035\n",
      "epoch complete: 201 cost: 410.9295825958252 dt: 0:00:07.854954\n",
      "epoch complete: 202 cost: 410.38692593574524 dt: 0:00:07.758306\n",
      "epoch complete: 203 cost: 416.4981380701065 dt: 0:00:07.967258\n",
      "epoch complete: 204 cost: 423.617078602314 dt: 0:00:07.805345\n",
      "epoch complete: 205 cost: 416.517726957798 dt: 0:00:07.822417\n",
      "epoch complete: 206 cost: 417.36920976638794 dt: 0:00:07.853000\n",
      "epoch complete: 207 cost: 410.3103128671646 dt: 0:00:07.801667\n",
      "epoch complete: 208 cost: 407.9736605286598 dt: 0:00:07.898815\n",
      "epoch complete: 209 cost: 406.9657157063484 dt: 0:00:08.592237\n",
      "epoch complete: 210 cost: 413.16395223140717 dt: 0:00:08.217847\n",
      "epoch complete: 211 cost: 410.2919805049896 dt: 0:00:07.825457\n",
      "epoch complete: 212 cost: 413.79433596134186 dt: 0:00:07.771565\n",
      "epoch complete: 213 cost: 413.95759880542755 dt: 0:00:07.896572\n",
      "epoch complete: 214 cost: 419.42904353141785 dt: 0:00:07.896104\n",
      "epoch complete: 215 cost: 410.73404824733734 dt: 0:00:07.840239\n",
      "epoch complete: 216 cost: 413.94403100013733 dt: 0:00:07.908720\n",
      "epoch complete: 217 cost: 411.28897500038147 dt: 0:00:07.888139\n",
      "epoch complete: 218 cost: 412.6054182648659 dt: 0:00:07.781391\n",
      "epoch complete: 219 cost: 426.5657929778099 dt: 0:00:07.782066\n",
      "epoch complete: 220 cost: 415.9161525964737 dt: 0:00:08.042054\n",
      "epoch complete: 221 cost: 414.89737969636917 dt: 0:00:07.919371\n",
      "epoch complete: 222 cost: 413.9475746154785 dt: 0:00:07.942715\n",
      "epoch complete: 223 cost: 406.2274952530861 dt: 0:00:07.972196\n",
      "epoch complete: 224 cost: 406.87556034326553 dt: 0:00:07.750157\n",
      "epoch complete: 225 cost: 411.40187430381775 dt: 0:00:07.896332\n",
      "epoch complete: 226 cost: 418.21796321868896 dt: 0:00:07.898107\n",
      "epoch complete: 227 cost: 417.18659126758575 dt: 0:00:07.903572\n",
      "epoch complete: 228 cost: 412.74137699604034 dt: 0:00:07.804949\n",
      "epoch complete: 229 cost: 408.9240391254425 dt: 0:00:07.666863\n",
      "epoch complete: 230 cost: 415.31534254550934 dt: 0:00:08.033095\n",
      "epoch complete: 231 cost: 415.56969743967056 dt: 0:00:09.727367\n",
      "epoch complete: 232 cost: 405.09436017274857 dt: 0:00:09.621957\n",
      "epoch complete: 233 cost: 409.07801669836044 dt: 0:00:10.993798\n",
      "epoch complete: 234 cost: 412.7014839053154 dt: 0:00:09.299677\n",
      "epoch complete: 235 cost: 411.0362865328789 dt: 0:00:08.832294\n",
      "epoch complete: 236 cost: 414.8997178077698 dt: 0:00:08.691407\n",
      "epoch complete: 237 cost: 405.1437282562256 dt: 0:00:09.097346\n",
      "epoch complete: 238 cost: 413.6928446292877 dt: 0:00:09.315893\n",
      "epoch complete: 239 cost: 404.9358300566673 dt: 0:00:09.006057\n",
      "epoch complete: 240 cost: 409.2134845852852 dt: 0:00:08.859019\n",
      "epoch complete: 241 cost: 411.19930493831635 dt: 0:00:08.784617\n",
      "epoch complete: 242 cost: 409.63296592235565 dt: 0:00:08.800186\n",
      "epoch complete: 243 cost: 411.51351577043533 dt: 0:00:08.826533\n",
      "epoch complete: 244 cost: 414.078052341938 dt: 0:00:08.769830\n",
      "epoch complete: 245 cost: 410.34134858846664 dt: 0:00:09.138722\n",
      "epoch complete: 246 cost: 408.4188035726547 dt: 0:00:08.645829\n",
      "epoch complete: 247 cost: 414.660901427269 dt: 0:00:08.747423\n",
      "epoch complete: 248 cost: 408.2551506757736 dt: 0:00:08.688294\n",
      "epoch complete: 249 cost: 407.50602078437805 dt: 0:00:08.699285\n",
      "epoch complete: 250 cost: 407.1471852660179 dt: 0:00:08.688510\n",
      "epoch complete: 251 cost: 409.15094608068466 dt: 0:00:08.910672\n",
      "epoch complete: 252 cost: 411.1055864095688 dt: 0:00:08.886303\n",
      "epoch complete: 253 cost: 411.46017265319824 dt: 0:00:08.746415\n",
      "epoch complete: 254 cost: 416.61995536088943 dt: 0:00:08.778536\n",
      "epoch complete: 255 cost: 410.0011643767357 dt: 0:00:08.605821\n",
      "epoch complete: 256 cost: 415.51481026411057 dt: 0:00:08.756407\n",
      "epoch complete: 257 cost: 412.56989628076553 dt: 0:00:08.592644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch complete: 258 cost: 413.6108285188675 dt: 0:00:08.770186\n",
      "epoch complete: 259 cost: 412.18625915050507 dt: 0:00:08.216409\n",
      "epoch complete: 260 cost: 407.3094576597214 dt: 0:00:08.178582\n",
      "epoch complete: 261 cost: 413.4641752243042 dt: 0:00:08.623158\n",
      "epoch complete: 262 cost: 413.42998218536377 dt: 0:00:08.705777\n",
      "epoch complete: 263 cost: 407.9589465856552 dt: 0:00:08.692942\n",
      "epoch complete: 264 cost: 415.6742743253708 dt: 0:00:08.573580\n",
      "epoch complete: 265 cost: 407.69612115621567 dt: 0:00:08.255958\n",
      "epoch complete: 266 cost: 409.4744441509247 dt: 0:00:08.324422\n",
      "epoch complete: 267 cost: 404.27536195516586 dt: 0:00:08.776961\n",
      "epoch complete: 268 cost: 409.25453251600266 dt: 0:00:08.647680\n",
      "epoch complete: 269 cost: 409.8841370344162 dt: 0:00:09.415963\n",
      "epoch complete: 270 cost: 415.7685214281082 dt: 0:00:08.634868\n",
      "epoch complete: 271 cost: 410.7929188013077 dt: 0:00:08.694637\n",
      "epoch complete: 272 cost: 411.8425949215889 dt: 0:00:08.650837\n",
      "epoch complete: 273 cost: 413.88881546258926 dt: 0:00:08.710633\n",
      "epoch complete: 274 cost: 414.1548252105713 dt: 0:00:08.653546\n",
      "epoch complete: 275 cost: 410.64279198646545 dt: 0:00:08.900045\n",
      "epoch complete: 276 cost: 411.8934463262558 dt: 0:00:08.410461\n",
      "epoch complete: 277 cost: 413.17829209566116 dt: 0:00:08.138418\n",
      "epoch complete: 278 cost: 410.55317598581314 dt: 0:00:08.147558\n",
      "epoch complete: 279 cost: 409.8785591721535 dt: 0:00:08.080332\n",
      "epoch complete: 280 cost: 409.9219107031822 dt: 0:00:08.049661\n",
      "epoch complete: 281 cost: 406.3550814986229 dt: 0:00:07.719951\n",
      "epoch complete: 282 cost: 412.9528098702431 dt: 0:00:07.748526\n",
      "epoch complete: 283 cost: 409.25063824653625 dt: 0:00:08.295284\n",
      "epoch complete: 284 cost: 416.6133232116699 dt: 0:00:08.666871\n",
      "epoch complete: 285 cost: 409.5867604613304 dt: 0:00:09.445640\n",
      "epoch complete: 286 cost: 419.8012908101082 dt: 0:00:09.123725\n",
      "epoch complete: 287 cost: 409.2611243724823 dt: 0:00:09.140604\n",
      "epoch complete: 288 cost: 409.80229222774506 dt: 0:00:08.500659\n",
      "epoch complete: 289 cost: 406.19353914260864 dt: 0:00:08.571616\n",
      "epoch complete: 290 cost: 407.24308025836945 dt: 0:00:08.750190\n",
      "epoch complete: 291 cost: 404.7842945456505 dt: 0:00:08.654368\n",
      "epoch complete: 292 cost: 415.4907868504524 dt: 0:00:08.563160\n",
      "epoch complete: 293 cost: 408.8751153945923 dt: 0:00:08.634892\n",
      "epoch complete: 294 cost: 407.7723407149315 dt: 0:00:08.594168\n",
      "epoch complete: 295 cost: 407.29092663526535 dt: 0:00:08.987212\n",
      "epoch complete: 296 cost: 411.70064228773117 dt: 0:00:08.892526\n",
      "epoch complete: 297 cost: 403.43431729078293 dt: 0:00:09.180280\n",
      "epoch complete: 298 cost: 417.92698633670807 dt: 0:00:08.631966\n",
      "epoch complete: 299 cost: 410.4962393641472 dt: 0:00:09.685402\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "directory = logdir\n",
    "#get the data\n",
    "paths, title2indx = get_data()\n",
    "    \n",
    "# number fo different titles\n",
    "number_of_titles = len(title2indx)\n",
    "\n",
    "# configuration parameters for skipgram\n",
    "context_size = 3\n",
    "num_negatives = 5 #number of negative sampling titles\n",
    "epochs = 300;\n",
    "D = 50 # word embedding size, as in NxD\n",
    "\n",
    "#distribution for drawing negative samples\n",
    "p_neg = get_negative_sampling_distribution(paths)\n",
    "    \n",
    "# Parameters\n",
    "W = np.random.randn(number_of_titles, D).astype(np.float32) # input-to-hidden\n",
    "V = np.random.randn(D, number_of_titles).astype(np.float32) #hidden-to-output\n",
    "    \n",
    "#create the model\n",
    "with tf.name_scope(\"Title2Vec_place_holders\"):\n",
    "    tf_input = tf.placeholder(tf.int32, shape=(None,),name=\"Input\")\n",
    "    tf_negtitle = tf.placeholder(tf.int32, shape=(None,), name=\"Negative_Title\")\n",
    "    tf_context = tf.placeholder(tf.int32, shape=(None,), name=\"Targets/context\") #targets (context)\n",
    "tfW = tf.Variable(W,name=\"W\")\n",
    "tfV = tf.Variable(V.T, name=\"V_transposed\")\n",
    "    \n",
    "def dot(A, B):\n",
    "    C = A*B\n",
    "    return tf.reduce_sum(C, axis=1)\n",
    "    \n",
    "#correct middle title output\n",
    "emb_input = tf.nn.embedding_lookup(tfW, tf_input) # 1xD\n",
    "emb_output = tf.nn.embedding_lookup(tfV, tf_context) #NxD\n",
    "correct_output = dot(emb_input, emb_output) # N\n",
    "    \n",
    "pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones(tf.shape(correct_output)), logits=correct_output)\n",
    "    \n",
    "# incorrect middle title output. incorrect middle title comes from negative sampling\n",
    "emb_input = tf.nn.embedding_lookup(tfW, tf_negtitle)\n",
    "incorrect_output = dot(emb_input, emb_output)\n",
    "    \n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros(tf.shape(incorrect_output)), logits=incorrect_output)\n",
    "    \n",
    "#total loss\n",
    "loss = tf.reduce_mean(pos_loss) + tf.reduce_mean(neg_loss)\n",
    "    \n",
    "# Optimizer\n",
    "train_op = tf.train.MomentumOptimizer(0.1, momentum=0.9).minimize(loss)\n",
    "    \n",
    "#make tensor flow session\n",
    "sess = tf.InteractiveSession() #interactive so i can keep it on more cells\n",
    "init_op = tf.global_variables_initializer()\n",
    "    \n",
    "#set up tensorboard\n",
    "loss_summary = tf.summary.scalar('Loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "sess.run(init_op)\n",
    "    \n",
    "# A list to save the cost to be plotted later\n",
    "#.   This will eventually be replced by tensor board stuff\n",
    "costs = []\n",
    "    \n",
    "# number of total titles in corpus\n",
    "total_titles = sum(len(path) for path in paths)\n",
    "print(\"Total number of titles:\", total_titles)\n",
    "  \n",
    "# for subsampling each sentence\n",
    "threshold = 1e-5\n",
    "p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "    \n",
    "#train the model\n",
    "for epoch in range(epochs):\n",
    "    #randomize what order we see the paths\n",
    "    np.random.shuffle(paths)\n",
    "        \n",
    "    #accumulate the cost\n",
    "    cost = 0;\n",
    "    counter = 0;\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    negtitles = []\n",
    "    t0 = datetime.now()\n",
    "    for path in paths:\n",
    "        #keep only some of the titles based on p_neg, this is for speeding up the training. We may need to play around with this to see if it is really helping.\n",
    "        path = [t for t in path if np.random.random() < (1 - p_drop[t])]\n",
    "        if len(path) < 2: #if the path is too short to be useful, move on to the next path.\n",
    "            continue\n",
    "            \n",
    "        #pick randomly which order to see the middle titles so we don't always see samples in the same order\n",
    "        randomly_ordered_positions = np.random.choice(len(path),\n",
    "                                                         size=len(path),\n",
    "                                                         replace=False)\n",
    "            \n",
    "        for j, pos in enumerate(randomly_ordered_positions):\n",
    "            # title is the \"input title\" that we will find the context titles around.\n",
    "            title = path[pos]\n",
    "                \n",
    "            #get the positive context titles/negative samples\n",
    "            context_titles = get_context(pos, path, context_size)\n",
    "            neg_title = np.random.choice(number_of_titles, p=p_neg) #this is a single negative middle word\n",
    "            #the negative middle word is put with the context words for the negative samples\n",
    "                \n",
    "            n = len(context_titles)\n",
    "            inputs += [title]*n\n",
    "            negtitles += [neg_title]*n\n",
    "            targets += context_titles\n",
    "\n",
    "        if len(inputs) >= 64:    #batch size of 64\n",
    "                \n",
    "                # Run one iteration of the network.                 \n",
    "            _, c = sess.run((train_op, loss),\n",
    "                            feed_dict={tf_input: inputs,\n",
    "                                       tf_negtitle: negtitles,\n",
    "                                       tf_context: targets})\n",
    "            cost += c\n",
    "              \n",
    "            #reset the batch\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            negtitles = []\n",
    "                    \n",
    "        counter += 1\n",
    "#         if counter % 100 == 0:\n",
    "#                 #may want to replace this with tensorboard stuff\n",
    "#             print(\"processed %s / %s\\r\" % (counter, len(paths)))    \n",
    "                \n",
    "    # print stuff so we know things are working\n",
    "    dt = datetime.now() - t0\n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "            \n",
    "    #save the cost\n",
    "    costs.append(cost)\n",
    "        \n",
    "    #tensor board update\n",
    "    summary_str = loss_summary.eval(session=sess, feed_dict={loss:cost})\n",
    "    file_writer.add_summary(summary_str, epoch)\n",
    "            \n",
    "#plot the cost per iteration\n",
    "plot.plot(range(len(costs)), costs)\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.ylabel(\"Cost\")\n",
    "plot.show()\n",
    "        \n",
    "#get parameters\n",
    "W, VT = sess.run((tfW, tfV))\n",
    "V = VT.T   \n",
    "\n",
    "#save the model\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "            \n",
    "with open('%s/title2indx.json' % directory, 'w') as f:\n",
    "    json.dump(title2indx, f)\n",
    "            \n",
    "np.savez('%s/weights.npz' % directory, W, V)\n",
    "        \n",
    "# #return the model\n",
    "# return title2indx, W, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard embedding visualization\n",
    "The cells below are my attempt to get the 3d representation of the title embeddings in tensorboard. I have not been very successful in this attempt, so the cells below don't have to be run. Also note that at the time of writing this, the things that do show up in tensorboard are labeled incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_shape = (tf.cast(W.shape[0], tf.int32), tf.cast(W.shape[1],tf.int32))\n",
    "embedding_var = tf.Variable(tf.zeros(embedding_shape ), \n",
    "                            name='W_embedding')\n",
    "# assign the tensor that we want to visualize to the embedding variable\n",
    "embedding_assign = embedding_var.assign(tfW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Create a config object to write the configuration parameters\n",
    "config = projector.ProjectorConfig()\n",
    "config.model_checkpoint_path = os.path.join(directory, \"model.ckpt\")\n",
    "\n",
    "# Add embedding variable\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# # Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n",
    "# embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "# # Specify where you find the sprite. -> we will create this image later\n",
    "# embedding.sprite.image_path = 'sprite_images.png'\n",
    "# embedding.sprite.single_image_dim.extend([img_w, img_h])\n",
    "\n",
    "# Write a projector_config.pbtxt in the logs_path.\n",
    "# TensorBoard will read this file during startup.\n",
    "projector.visualize_embeddings(file_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n",
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# a cell I have been using for debugging\n",
    "print(tf_context.shape)\n",
    "type(W)\n",
    "print(W.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorFlow_logs/run-20181212_2345/model.ckpt-300'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run session to evaluate the tensor\n",
    "w_emb = sess.run(embedding_assign, feed_dict={tf_context:targets})\n",
    "\n",
    "# Save the tensor in model.ckpt file\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(directory, \"model.ckpt\"), epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where I run the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths/title2indx.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6fb263096b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f9b3690aacb5>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/title2indx.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtitle2indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnpz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/weights.npz'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths/title2indx.json'"
     ]
    }
   ],
   "source": [
    "directory = \"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths\"\n",
    "\n",
    "t, w, v = load_model(directory)\n",
    "test_model(t, w, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(w))\n",
    "print(w.shape)\n",
    "print(w[t['intern']])\n",
    "\n",
    "print(type(v))\n",
    "print(v.shape)\n",
    "print(v[t['intern']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the session!\n",
    "This needs to be run after the training cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
