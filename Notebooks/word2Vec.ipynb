{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.special import expit as sigmoid\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory\n",
    "The cell below creates a directory for the tensorboard files to go into as well as where the weights and title2indx dictionary will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\");\n",
    "root_logdir = \"tensorFlow_logs\";\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data function\n",
    "The cell below defines how the data is read in and sets up the input for the rest of the model.\n",
    "\n",
    "**Adjustable Parameters**:\n",
    "* V - the number of elements or dimensions used in the word embedding. I found that 2000 gave decent results but that tensorboard's PCA calculations couldn't handle that big of embeddings.\n",
    "* fileLoc - The location of the input file. **This must be changed for your set up!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    s = s.replace(',', '')\n",
    "    s = s.replace('.', '')\n",
    "    s = s.replace('\\'', '')\n",
    "    s = s.replace('\\\"', '')\n",
    "    return s\n",
    "\n",
    "def get_data():\n",
    "    V = 8000   #The max number of unique job titles. \n",
    "                #This essentially defines the size of the feature space.\n",
    "    fileLoc = \"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/allPaths.txt\"\n",
    "    file = open(fileLoc)\n",
    "    all_titles_counts = {} #dictionary of job titles and thier counts\n",
    "    \n",
    "    # Count the total number of titles\n",
    "    for line in file:\n",
    "        path = remove_punctuation(line).split()\n",
    "        if len(path) > 1:\n",
    "            for title in path:\n",
    "                if title not in all_titles_counts:\n",
    "                    all_titles_counts[title] = 0\n",
    "                all_titles_counts[title] += 1\n",
    "    print(\"finished counting titles\")\n",
    "    \n",
    "    #if there are fewer titles than V, reassign it.\n",
    "    V = min(len(all_titles_counts), V)\n",
    "    # sort the dictionary by the number of times a title appears.\n",
    "    all_titles_counts = sorted(all_titles_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #get the top V number of titles but reserve one for the \"unknowns\" which\n",
    "    # is really just for anything that didn't show up enough to make the list.\n",
    "    top_titles = [t for t, count in all_titles_counts[:V-1]] + ['<UNK>']\n",
    "    #create a mapping from title to index for top_titles.\n",
    "    title2indx = {t:i for i, t in enumerate(top_titles)}\n",
    "    unk = title2indx['<UNK>']\n",
    "    print(\"Index for the 'unknowns' is : \" + str(unk))\n",
    "    \n",
    "    paths = [] #This will be a list of vectorized job paths.\n",
    "    for line in open(fileLoc):\n",
    "        path = remove_punctuation(line).split() # 'path' is the string of the path\n",
    "        if len(path) > 1:    #only a path with two job titles will give any context\n",
    "            p = [title2indx[t] if t in title2indx else unk for t in path] # 'p' is the vectorized path\n",
    "            paths.append(p)\n",
    "            \n",
    "    print(paths[:3])\n",
    "    print(\"index for 'software_engineer': \" + str(title2indx[\"software_engineer\"]))\n",
    "    print(\"number of paths from get data: \" + str(len(paths)))\n",
    "    return paths, title2indx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling distribution\n",
    "The function below defines the distribution for negative sampling. Negative sampling is used in this code as an approximation to softmax. Negative sampling is reported to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative sampling is a faster way to approximate the softmax function when using skipgram\n",
    "def get_negative_sampling_distribution(paths):\n",
    "    # Pn(t) = prob of title occuring\n",
    "    # we would like to sample the negative samples such that\n",
    "    # titles that occur more often should be sampled more often\n",
    "    \n",
    "    title_freq = {}\n",
    "    title_count = sum(len(path) for path in paths)\n",
    "    for path in paths:\n",
    "        for title in path:\n",
    "            if title not in title_freq:\n",
    "                 title_freq[title] = 0\n",
    "            title_freq[title] += 1\n",
    "        \n",
    "    # vocab size\n",
    "    V = len(title_freq)\n",
    "        \n",
    "    p_neg = np.zeros(V)\n",
    "    for j in range(V):\n",
    "        p_neg[j] = title_freq[j]**0.75 #0.75 is an adjustable parameter.\n",
    "        # This comes form the modified uniform distribution.\n",
    "            \n",
    "    # normalize p_neg\n",
    "    p_neg = p_neg / p_neg.sum()\n",
    "        \n",
    "    assert(np.all(p_neg > 0))\n",
    "    return p_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get context\n",
    "The function below gets the context words for the skipgram model. Note that this function is modified just a bit from the regular skipgram model in that it only takes context words that come after the selected position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, path, context_size):\n",
    "    # input:\n",
    "    # a path of the form: x x x pos c c c x x x\n",
    "    #output:\n",
    "    # the context paths: c c c \n",
    "    \n",
    "#     start = max(0, pos - context_size) #this is if you want backward context as well.\n",
    "    start = pos # This is for forward context only. Like next job. This is something to play around with.\n",
    "    end_ = min(len(path), pos + context_size)\n",
    "    \n",
    "    context = []\n",
    "    for context_pos, context_title_indx in enumerate(path[start:end_], start=start):\n",
    "        if context_pos != pos:\n",
    "            #don't include the input title itself as a target\n",
    "            context.append(context_title_indx)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions\n",
    "The function below gives some indication of how well our model fits. Read it and the analogy function carefully to understand what it is reporting. More calls to analogy can be put into the test_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(title2indx, W, V):\n",
    "\n",
    "    indx2title = {i:t for t, i in title2indx.items()}\n",
    "        # I am taking the title embedding as the average of the two weight vectors\n",
    "    for word_embedding in (W, (W + V.T)/2):\n",
    "        print(\"_______Testing_________\")\n",
    "        analogy('research_intern', 'graduate_research_assistant', 'intern', 'research_assistant', title2indx, indx2title, W)\n",
    "        analogy('engineering_intern', 'engineer', 'software_engineering_intern', 'software_engineer', title2indx, indx2title, W)\n",
    "        analogy('software_engineer', 'software_developer', 'director', 'ceo', title2indx, indx2title, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive1, negative1, positive2, negative2, title2indx, indx2title, W):\n",
    "    V, D = W.shape\n",
    "    \n",
    "    print(\"--------------\\ntesting: %s - %s = %s - %s\" %(positive1, negative1, positive2, negative2))\n",
    "    for t in (positive1, negative1, positive2, negative2):\n",
    "        if t not in (title2indx):\n",
    "            print(t + \" is not in the used titles.\")\n",
    "            \n",
    "    p1 = W[title2indx[positive1]]\n",
    "    p2 = W[title2indx[positive2]]\n",
    "    n1 = W[title2indx[negative1]]\n",
    "    n2 = W[title2indx[negative2]]\n",
    "    \n",
    "    vec = p1 - n1 + n2\n",
    "    \n",
    "    #using the cosine distance rather than the euclidian distance.\n",
    "    distances = pairwise_distances(vec.reshape(1,D), W, metric='cosine').reshape(V)\n",
    "    indx = distances.argsort()[:5] #gets the index of the 5 closest vectors\n",
    "    bestIndx = -1\n",
    "    # remove the input titles as possibilities\n",
    "    keep_out = [title2indx[t] for t in (positive1, negative1, negative2)]\n",
    "    for r in indx:\n",
    "        if r not in keep_out:\n",
    "            bestIndx = r;\n",
    "            break\n",
    "     \n",
    "    for i in indx:\n",
    "        print(indx2title[i], distances[i])\n",
    "        \n",
    "    print(\"Distance to %s: \" % positive2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "The model can take a long time to run, so when running the model, a number of files are saved in order to avoid needing to run the model multiple times. The Load model will read those files and give back the title2indx dictionary as well as the hidden layer weights. I have typically been using (W + V.T)/2 as the official title embedding, as seen in the test_model function. This can be changed based on future testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(directory):\n",
    "    with open('%s/title2indx.json' % directory) as f:\n",
    "        title2indx = json.load(f)\n",
    "    npz = np.load('%s/weights.npz' % directory)\n",
    "    W = npz['arr_0']\n",
    "    V = npz['arr_1']\n",
    "    return title2indx, W, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "__DO NOT RUN THIS CELL WITHOUT READING THIS FIRST!__\n",
    "\n",
    "This is where the magic happens! There are a number of adjustable parameters that can be tweaked as described below. This cell can take a long time to run. This is of course dependant on other parameters as well so make sure you have the parameters set the way you want them.\n",
    "\n",
    "In this cell I am using tensorflow's \"Interactive Session\" which needs to be closed explicitly. The closing statment is the bottom cell. The interactive session can cause some wierd things to happen if it is left open and/or run out of order. Look [here](https://www.tensorflow.org/api_docs/python/tf/InteractiveSession) for details.\n",
    "\n",
    "**Adjustable parameters:**\n",
    "* context_size - The size of the window used to get context words. This is a little different in this code than normal. see the get_context function above.\n",
    "* learning_rate - the starting learning rate\n",
    "* final_learning_rate - the final learning rate. The learning rate will change until it reaches this value.\n",
    "* num_negatives - number of negative samples to use. see [this site](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) for more of a description of negative sampling.\n",
    "* epochs - the number of training rounds to go through. So far, I have found that this has the largest impact on run time. I have found 300 epochs gives decent results.\n",
    "* D - the word embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting titles\n",
      "Index for the 'unknowns' is : 7999\n",
      "[[463, 51, 1, 1, 1], [7999, 7999, 135, 2854], [62, 8, 394, 2, 394, 394]]\n",
      "index for 'software_engineer': 1\n",
      "number of paths from get data: 892370\n",
      "Total number of titles: 5750593\n",
      "epoch complete: 0 cost: 14263.054271697998 dt: 0:00:40.762076\n",
      "epoch complete: 1 cost: 11591.718243718147 dt: 0:00:37.697352\n",
      "epoch complete: 2 cost: 10079.182488918304 dt: 0:00:38.379445\n",
      "epoch complete: 3 cost: 8985.189368724823 dt: 0:00:38.025077\n",
      "epoch complete: 4 cost: 8296.786078333855 dt: 0:00:39.079301\n",
      "epoch complete: 5 cost: 7661.2118228673935 dt: 0:00:38.681709\n",
      "epoch complete: 6 cost: 7267.848479390144 dt: 0:00:38.708788\n",
      "epoch complete: 7 cost: 6914.655648112297 dt: 0:00:37.617018\n",
      "epoch complete: 8 cost: 6594.991555809975 dt: 0:00:38.611601\n",
      "epoch complete: 9 cost: 6293.484773755074 dt: 0:00:37.755865\n",
      "epoch complete: 10 cost: 6111.813556075096 dt: 0:00:37.590716\n",
      "epoch complete: 11 cost: 5896.009659767151 dt: 0:00:38.597850\n",
      "epoch complete: 12 cost: 5732.705411791801 dt: 0:00:37.697331\n",
      "epoch complete: 13 cost: 5559.4972331523895 dt: 0:00:38.058581\n",
      "epoch complete: 14 cost: 5426.98012816906 dt: 0:00:38.506201\n",
      "epoch complete: 15 cost: 5282.533203601837 dt: 0:00:38.713472\n",
      "epoch complete: 16 cost: 5163.718627870083 dt: 0:00:38.544398\n",
      "epoch complete: 17 cost: 5063.614410638809 dt: 0:00:38.506445\n",
      "epoch complete: 18 cost: 4966.604154825211 dt: 0:00:37.548495\n",
      "epoch complete: 19 cost: 4878.819498300552 dt: 0:00:37.585052\n",
      "epoch complete: 20 cost: 4772.618364036083 dt: 0:00:37.729970\n",
      "epoch complete: 21 cost: 4689.925501585007 dt: 0:00:40.552617\n",
      "epoch complete: 22 cost: 4592.010092377663 dt: 0:00:40.331407\n",
      "epoch complete: 23 cost: 4552.070920646191 dt: 0:00:39.788049\n",
      "epoch complete: 24 cost: 4477.13861232996 dt: 0:00:37.390937\n",
      "epoch complete: 25 cost: 4419.637310683727 dt: 0:00:37.740512\n",
      "epoch complete: 26 cost: 4353.23471313715 dt: 0:00:37.453042\n",
      "epoch complete: 27 cost: 4296.853094577789 dt: 0:00:37.744338\n",
      "epoch complete: 28 cost: 4200.983827590942 dt: 0:00:37.590216\n",
      "epoch complete: 29 cost: 4156.364045739174 dt: 0:00:37.419619\n",
      "epoch complete: 30 cost: 4091.55942094326 dt: 0:00:37.767394\n",
      "epoch complete: 31 cost: 4057.232300877571 dt: 0:00:37.534937\n",
      "epoch complete: 32 cost: 3998.2759642004967 dt: 0:00:37.487357\n",
      "epoch complete: 33 cost: 3953.0712069272995 dt: 0:00:37.606501\n",
      "epoch complete: 34 cost: 3878.5433047413826 dt: 0:00:39.062982\n",
      "epoch complete: 35 cost: 3823.214059114456 dt: 0:00:40.429130\n",
      "epoch complete: 36 cost: 3796.2800087332726 dt: 0:00:37.372090\n",
      "epoch complete: 37 cost: 3731.027195572853 dt: 0:00:37.455932\n",
      "epoch complete: 38 cost: 3734.6448239684105 dt: 0:00:37.803491\n",
      "epoch complete: 39 cost: 3661.6111422777176 dt: 0:00:37.381475\n",
      "epoch complete: 40 cost: 3627.700514614582 dt: 0:00:37.268100\n",
      "epoch complete: 41 cost: 3584.7159463763237 dt: 0:00:37.326387\n",
      "epoch complete: 42 cost: 3557.8007922172546 dt: 0:00:37.556094\n",
      "epoch complete: 43 cost: 3538.7055136561394 dt: 0:00:37.602078\n",
      "epoch complete: 44 cost: 3518.1761930286884 dt: 0:00:37.545184\n",
      "epoch complete: 45 cost: 3460.422788321972 dt: 0:00:37.357227\n",
      "epoch complete: 46 cost: 3442.7879191040993 dt: 0:00:37.482603\n",
      "epoch complete: 47 cost: 3394.7182974517345 dt: 0:00:37.551065\n",
      "epoch complete: 48 cost: 3390.3061289191246 dt: 0:00:37.466048\n",
      "epoch complete: 49 cost: 3385.1047953665257 dt: 0:00:37.956526\n",
      "epoch complete: 50 cost: 3310.7798073887825 dt: 0:00:37.558962\n",
      "epoch complete: 51 cost: 3313.442240536213 dt: 0:00:37.499993\n",
      "epoch complete: 52 cost: 3268.122626438737 dt: 0:00:37.391495\n",
      "epoch complete: 53 cost: 3239.3337498903275 dt: 0:00:37.469330\n",
      "epoch complete: 54 cost: 3238.7266008257866 dt: 0:00:39.643489\n",
      "epoch complete: 55 cost: 3189.844750583172 dt: 0:00:38.755090\n",
      "epoch complete: 56 cost: 3192.2900154441595 dt: 0:00:37.920343\n",
      "epoch complete: 57 cost: 3195.6090472638607 dt: 0:00:37.983941\n",
      "epoch complete: 58 cost: 3141.2204593122005 dt: 0:00:37.322150\n",
      "epoch complete: 59 cost: 3140.3985470980406 dt: 0:00:37.488776\n",
      "epoch complete: 60 cost: 3088.3468468487263 dt: 0:00:37.751802\n",
      "epoch complete: 61 cost: 3072.984601855278 dt: 0:00:37.326284\n",
      "epoch complete: 62 cost: 3077.8002260923386 dt: 0:00:37.655886\n",
      "epoch complete: 63 cost: 3056.1243204176426 dt: 0:00:37.881858\n",
      "epoch complete: 64 cost: 3027.0184337347746 dt: 0:00:38.323597\n",
      "epoch complete: 65 cost: 3010.0281744003296 dt: 0:00:37.708691\n",
      "epoch complete: 66 cost: 3004.1575364619493 dt: 0:00:37.471375\n",
      "epoch complete: 67 cost: 2982.7300642728806 dt: 0:00:37.865812\n",
      "epoch complete: 68 cost: 2973.226063787937 dt: 0:00:37.949855\n",
      "epoch complete: 69 cost: 2962.7716770619154 dt: 0:00:37.914747\n",
      "epoch complete: 70 cost: 2951.236729681492 dt: 0:00:38.952583\n",
      "epoch complete: 71 cost: 2908.594930768013 dt: 0:00:37.486171\n",
      "epoch complete: 72 cost: 2925.2876481711864 dt: 0:00:38.452468\n",
      "epoch complete: 73 cost: 2900.8287312984467 dt: 0:00:38.014084\n",
      "epoch complete: 74 cost: 2884.0426363497972 dt: 0:00:37.572613\n",
      "epoch complete: 75 cost: 2851.747098147869 dt: 0:00:37.510952\n",
      "epoch complete: 76 cost: 2852.922123581171 dt: 0:00:37.513239\n",
      "epoch complete: 77 cost: 2839.657835960388 dt: 0:00:37.476459\n",
      "epoch complete: 78 cost: 2839.2628639936447 dt: 0:00:37.496289\n",
      "epoch complete: 79 cost: 2821.987686932087 dt: 0:00:37.566241\n",
      "epoch complete: 80 cost: 2802.8562911897898 dt: 0:00:37.248755\n",
      "epoch complete: 81 cost: 2806.783820837736 dt: 0:00:37.330897\n",
      "epoch complete: 82 cost: 2793.241284698248 dt: 0:00:37.800208\n",
      "epoch complete: 83 cost: 2786.6915117949247 dt: 0:00:37.534710\n",
      "epoch complete: 84 cost: 2776.9483055993915 dt: 0:00:37.609041\n",
      "epoch complete: 85 cost: 2770.7927084565163 dt: 0:00:37.584235\n",
      "epoch complete: 86 cost: 2747.1658130660653 dt: 0:00:37.439651\n",
      "epoch complete: 87 cost: 2727.6538005173206 dt: 0:00:37.504778\n",
      "epoch complete: 88 cost: 2729.8485056087375 dt: 0:00:37.483079\n",
      "epoch complete: 89 cost: 2728.652212589979 dt: 0:00:38.086384\n",
      "epoch complete: 90 cost: 2707.536067098379 dt: 0:00:37.645475\n",
      "epoch complete: 91 cost: 2700.500874876976 dt: 0:00:37.446136\n",
      "epoch complete: 92 cost: 2697.664444208145 dt: 0:00:37.992650\n",
      "epoch complete: 93 cost: 2688.764416515827 dt: 0:00:39.324842\n",
      "epoch complete: 94 cost: 2699.459580332041 dt: 0:00:40.012783\n",
      "epoch complete: 95 cost: 2657.048710167408 dt: 0:00:41.980534\n",
      "epoch complete: 96 cost: 2666.599629163742 dt: 0:00:39.177918\n",
      "epoch complete: 97 cost: 2675.6371250376105 dt: 0:00:39.828210\n",
      "epoch complete: 98 cost: 2666.767897799611 dt: 0:00:38.891305\n",
      "epoch complete: 99 cost: 2637.12448823452 dt: 0:00:39.469961\n",
      "epoch complete: 100 cost: 2642.7436598837376 dt: 0:00:40.967644\n",
      "epoch complete: 101 cost: 2633.626689016819 dt: 0:00:39.694995\n",
      "epoch complete: 102 cost: 2625.261727511883 dt: 0:00:40.242859\n",
      "epoch complete: 103 cost: 2609.6786493062973 dt: 0:00:41.199388\n",
      "epoch complete: 104 cost: 2603.2848773747683 dt: 0:00:40.451128\n",
      "epoch complete: 105 cost: 2596.095774181187 dt: 0:00:40.898050\n",
      "epoch complete: 106 cost: 2598.1049632430077 dt: 0:00:40.153827\n",
      "epoch complete: 107 cost: 2600.6365185827017 dt: 0:00:40.309062\n",
      "epoch complete: 108 cost: 2600.259881824255 dt: 0:00:40.456130\n",
      "epoch complete: 109 cost: 2575.436170488596 dt: 0:00:40.430586\n",
      "epoch complete: 110 cost: 2574.0203137397766 dt: 0:00:40.250905\n",
      "epoch complete: 111 cost: 2571.4287998974323 dt: 0:00:40.417507\n",
      "epoch complete: 112 cost: 2555.4391974657774 dt: 0:00:40.425846\n",
      "epoch complete: 113 cost: 2572.6686993837357 dt: 0:00:40.187760\n",
      "epoch complete: 114 cost: 2561.9003981649876 dt: 0:00:41.056421\n",
      "epoch complete: 115 cost: 2552.785473883152 dt: 0:00:41.897697\n",
      "epoch complete: 116 cost: 2562.0451727211475 dt: 0:00:42.773659\n",
      "epoch complete: 117 cost: 2565.469882450998 dt: 0:00:40.464774\n",
      "epoch complete: 118 cost: 2539.1524386405945 dt: 0:00:40.711000\n",
      "epoch complete: 119 cost: 2533.232856720686 dt: 0:00:38.328097\n",
      "epoch complete: 120 cost: 2524.6758956462145 dt: 0:00:37.607406\n",
      "epoch complete: 121 cost: 2525.158519640565 dt: 0:00:37.414370\n",
      "epoch complete: 122 cost: 2522.800278291106 dt: 0:00:37.492836\n",
      "epoch complete: 123 cost: 2520.72751429677 dt: 0:00:37.511098\n",
      "epoch complete: 124 cost: 2521.3900809288025 dt: 0:00:37.457018\n",
      "epoch complete: 125 cost: 2508.067445382476 dt: 0:00:37.891479\n",
      "epoch complete: 126 cost: 2499.586999103427 dt: 0:00:37.701546\n",
      "epoch complete: 127 cost: 2513.2724710591137 dt: 0:00:37.918884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch complete: 128 cost: 2512.675096347928 dt: 0:00:40.546443\n",
      "epoch complete: 129 cost: 2509.8489569872618 dt: 0:00:39.407920\n",
      "epoch complete: 130 cost: 2498.330734938383 dt: 0:00:37.620911\n",
      "epoch complete: 131 cost: 2478.5286532193422 dt: 0:00:40.059637\n",
      "epoch complete: 132 cost: 2468.813377894461 dt: 0:00:37.474677\n",
      "epoch complete: 133 cost: 2484.724829658866 dt: 0:00:43.741394\n",
      "epoch complete: 134 cost: 2478.872435197234 dt: 0:00:39.471787\n",
      "epoch complete: 135 cost: 2477.7723253294826 dt: 0:00:40.121316\n",
      "epoch complete: 136 cost: 2487.3842144161463 dt: 0:00:39.464876\n",
      "epoch complete: 137 cost: 2474.037821829319 dt: 0:00:39.504891\n",
      "epoch complete: 138 cost: 2470.818568378687 dt: 0:00:39.173514\n",
      "epoch complete: 139 cost: 2471.918444700539 dt: 0:00:40.330299\n",
      "epoch complete: 140 cost: 2468.7553960978985 dt: 0:00:40.361770\n",
      "epoch complete: 141 cost: 2446.146668344736 dt: 0:00:37.579958\n",
      "epoch complete: 142 cost: 2463.5546455085278 dt: 0:00:37.984244\n",
      "epoch complete: 143 cost: 2464.885893315077 dt: 0:00:38.170244\n",
      "epoch complete: 144 cost: 2456.53840880841 dt: 0:00:37.534202\n",
      "epoch complete: 145 cost: 2445.5316037237644 dt: 0:00:37.547151\n",
      "epoch complete: 146 cost: 2428.873772546649 dt: 0:00:37.581991\n",
      "epoch complete: 147 cost: 2434.5330834686756 dt: 0:00:37.313779\n",
      "epoch complete: 148 cost: 2436.634671241045 dt: 0:00:38.545405\n",
      "epoch complete: 149 cost: 2441.904759198427 dt: 0:00:39.327831\n",
      "epoch complete: 150 cost: 2431.6542898267508 dt: 0:00:38.879972\n",
      "epoch complete: 151 cost: 2423.753054022789 dt: 0:00:37.427045\n",
      "epoch complete: 152 cost: 2412.621770441532 dt: 0:00:37.654387\n",
      "epoch complete: 153 cost: 2420.6537123173475 dt: 0:00:39.270873\n",
      "epoch complete: 154 cost: 2413.6338024437428 dt: 0:00:38.518023\n",
      "epoch complete: 155 cost: 2424.5415958464146 dt: 0:00:40.823601\n",
      "epoch complete: 156 cost: 2416.48528444767 dt: 0:00:37.772297\n",
      "epoch complete: 157 cost: 2422.5238997340202 dt: 0:00:37.654634\n",
      "epoch complete: 158 cost: 2409.0326236262918 dt: 0:00:39.715986\n",
      "epoch complete: 159 cost: 2400.257942289114 dt: 0:00:39.506964\n",
      "epoch complete: 160 cost: 2411.209685333073 dt: 0:00:38.588579\n",
      "epoch complete: 161 cost: 2407.431024581194 dt: 0:00:39.596719\n",
      "epoch complete: 162 cost: 2393.2934162169695 dt: 0:00:39.508731\n",
      "epoch complete: 163 cost: 2405.089391246438 dt: 0:00:40.274007\n",
      "epoch complete: 164 cost: 2398.966358333826 dt: 0:00:39.053400\n",
      "epoch complete: 165 cost: 2397.849632024765 dt: 0:00:38.634347\n",
      "epoch complete: 166 cost: 2402.0719569027424 dt: 0:00:37.897234\n",
      "epoch complete: 167 cost: 2414.818438448012 dt: 0:00:39.572497\n",
      "epoch complete: 168 cost: 2394.3953482210636 dt: 0:00:38.658305\n",
      "epoch complete: 169 cost: 2369.3021922446787 dt: 0:00:38.380626\n",
      "epoch complete: 170 cost: 2396.529067158699 dt: 0:00:37.228349\n",
      "epoch complete: 171 cost: 2381.2950650155544 dt: 0:00:38.032413\n",
      "epoch complete: 172 cost: 2383.3236855864525 dt: 0:00:42.419992\n",
      "epoch complete: 173 cost: 2375.4682873934507 dt: 0:00:37.586208\n",
      "epoch complete: 174 cost: 2381.5605860948563 dt: 0:00:37.673336\n",
      "epoch complete: 175 cost: 2383.6964689046144 dt: 0:00:37.613281\n",
      "epoch complete: 176 cost: 2380.0800796300173 dt: 0:00:37.490778\n",
      "epoch complete: 177 cost: 2382.858998954296 dt: 0:00:37.462567\n",
      "epoch complete: 178 cost: 2361.8956893086433 dt: 0:00:37.506522\n",
      "epoch complete: 179 cost: 2378.6461897827685 dt: 0:00:37.559679\n",
      "epoch complete: 180 cost: 2369.2721894681454 dt: 0:00:37.674604\n",
      "epoch complete: 181 cost: 2373.924831569195 dt: 0:00:37.605923\n",
      "epoch complete: 182 cost: 2369.4600381851196 dt: 0:00:37.485234\n",
      "epoch complete: 183 cost: 2362.1272162795067 dt: 0:00:37.535102\n",
      "epoch complete: 184 cost: 2354.6292473077774 dt: 0:00:37.529143\n",
      "epoch complete: 185 cost: 2356.720322906971 dt: 0:00:37.429609\n",
      "epoch complete: 186 cost: 2367.247307546437 dt: 0:00:37.676746\n",
      "epoch complete: 187 cost: 2390.1596804857254 dt: 0:00:37.776647\n",
      "epoch complete: 188 cost: 2376.561100691557 dt: 0:00:38.024651\n",
      "epoch complete: 189 cost: 2343.739497460425 dt: 0:00:37.373929\n",
      "epoch complete: 190 cost: 2363.4899246245623 dt: 0:00:37.512330\n",
      "epoch complete: 191 cost: 2359.7396026551723 dt: 0:00:38.071110\n",
      "epoch complete: 192 cost: 2362.6002382040024 dt: 1:52:12.195547\n",
      "epoch complete: 193 cost: 2338.7888562232256 dt: 0:00:48.187172\n",
      "epoch complete: 194 cost: 2345.559172615409 dt: 0:00:44.336614\n",
      "epoch complete: 195 cost: 2344.4818586707115 dt: 0:00:40.628608\n",
      "epoch complete: 196 cost: 2349.235525727272 dt: 0:00:40.304101\n",
      "epoch complete: 197 cost: 2345.746801197529 dt: 0:00:40.120284\n",
      "epoch complete: 198 cost: 2334.7815930247307 dt: 0:00:41.577595\n",
      "epoch complete: 199 cost: 2331.0184281021357 dt: 0:00:38.398441\n",
      "epoch complete: 200 cost: 2339.367161422968 dt: 0:00:40.893235\n",
      "epoch complete: 201 cost: 2360.977132037282 dt: 0:00:42.074833\n",
      "epoch complete: 202 cost: 2334.690221309662 dt: 0:00:39.600512\n",
      "epoch complete: 203 cost: 2350.9898121505976 dt: 0:00:46.821220\n",
      "epoch complete: 204 cost: 2345.6419281363487 dt: 0:00:45.374933\n",
      "epoch complete: 205 cost: 2342.296777188778 dt: 0:00:41.928176\n",
      "epoch complete: 206 cost: 2330.447633266449 dt: 0:00:42.471173\n",
      "epoch complete: 207 cost: 2338.4129881858826 dt: 0:00:45.820321\n",
      "epoch complete: 208 cost: 2343.0432192236185 dt: 0:00:43.621871\n",
      "epoch complete: 209 cost: 2337.7066346108913 dt: 0:00:42.650736\n",
      "epoch complete: 210 cost: 2334.117205619812 dt: 0:00:42.253963\n",
      "epoch complete: 211 cost: 2328.3125058710575 dt: 0:00:41.313733\n",
      "epoch complete: 212 cost: 2332.1251935511827 dt: 0:00:43.893023\n",
      "epoch complete: 213 cost: 2314.883025549352 dt: 0:00:44.853795\n",
      "epoch complete: 214 cost: 2315.7218424379826 dt: 0:00:42.032339\n",
      "epoch complete: 215 cost: 2325.0677255764604 dt: 0:00:43.032785\n",
      "epoch complete: 216 cost: 2316.0069467276335 dt: 0:00:37.766063\n",
      "epoch complete: 217 cost: 2306.525252044201 dt: 0:00:37.369895\n",
      "epoch complete: 218 cost: 2319.575606971979 dt: 0:00:37.629232\n",
      "epoch complete: 219 cost: 2316.4696712493896 dt: 0:00:37.444748\n",
      "epoch complete: 220 cost: 2315.3428891450167 dt: 0:00:37.581349\n",
      "epoch complete: 221 cost: 2296.8040851950645 dt: 0:00:37.631563\n",
      "epoch complete: 222 cost: 2318.3670367598534 dt: 0:00:37.564864\n",
      "epoch complete: 223 cost: 2315.525345236063 dt: 0:00:37.741479\n",
      "epoch complete: 224 cost: 2307.5899869799614 dt: 0:00:37.595020\n",
      "epoch complete: 225 cost: 2314.7476945482194 dt: 0:00:37.447861\n",
      "epoch complete: 226 cost: 2314.3403185009956 dt: 0:00:37.630116\n",
      "epoch complete: 227 cost: 2306.8716534748673 dt: 0:00:37.417307\n",
      "epoch complete: 228 cost: 2303.6036233603954 dt: 0:00:37.605684\n",
      "epoch complete: 229 cost: 2312.1879571527243 dt: 0:00:37.874255\n",
      "epoch complete: 230 cost: 2296.0407575815916 dt: 0:00:37.467536\n",
      "epoch complete: 231 cost: 2311.0003949552774 dt: 0:00:37.648326\n",
      "epoch complete: 232 cost: 2303.044670626521 dt: 0:00:40.189143\n",
      "epoch complete: 233 cost: 2309.6338004916906 dt: 0:00:38.429753\n",
      "epoch complete: 234 cost: 2286.7005923986435 dt: 0:00:37.809965\n",
      "epoch complete: 235 cost: 2301.192221403122 dt: 0:00:37.735802\n",
      "epoch complete: 236 cost: 2288.7497260570526 dt: 0:00:37.372521\n",
      "epoch complete: 237 cost: 2290.7923029512167 dt: 0:00:37.350675\n",
      "epoch complete: 238 cost: 2291.0026745796204 dt: 0:00:37.472399\n",
      "epoch complete: 239 cost: 2284.397987395525 dt: 0:00:37.661402\n",
      "epoch complete: 240 cost: 2287.7369062900543 dt: 0:00:37.478327\n",
      "epoch complete: 241 cost: 2280.4938537478447 dt: 0:00:37.495857\n",
      "epoch complete: 242 cost: 2298.1455960273743 dt: 0:00:37.740548\n",
      "epoch complete: 243 cost: 2300.1271499693394 dt: 0:00:37.583366\n",
      "epoch complete: 244 cost: 2282.7074989378452 dt: 0:00:37.631785\n",
      "epoch complete: 245 cost: 2288.084332227707 dt: 0:00:37.840046\n",
      "epoch complete: 246 cost: 2280.1107737123966 dt: 0:00:37.739038\n",
      "epoch complete: 247 cost: 2302.4022616147995 dt: 0:00:37.737829\n",
      "epoch complete: 248 cost: 2276.0608583539724 dt: 0:00:37.740563\n",
      "epoch complete: 249 cost: 2289.0029057860374 dt: 0:00:37.687655\n",
      "epoch complete: 250 cost: 2274.405715763569 dt: 1:00:31.146706\n",
      "epoch complete: 251 cost: 2270.649022012949 dt: 0:00:47.407537\n",
      "epoch complete: 252 cost: 2287.574227437377 dt: 0:00:37.804640\n",
      "epoch complete: 253 cost: 2280.8647996485233 dt: 0:00:37.631040\n",
      "epoch complete: 254 cost: 2276.3212704993784 dt: 0:00:39.719671\n",
      "epoch complete: 255 cost: 2282.1059065908194 dt: 0:00:37.743857\n",
      "epoch complete: 256 cost: 2294.4298558086157 dt: 0:00:37.805634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch complete: 257 cost: 2285.629918590188 dt: 0:00:37.738871\n",
      "epoch complete: 258 cost: 2275.0923644304276 dt: 0:00:37.530220\n",
      "epoch complete: 259 cost: 2272.923779785633 dt: 0:00:37.494892\n",
      "epoch complete: 260 cost: 2278.5423054099083 dt: 0:00:37.658582\n",
      "epoch complete: 261 cost: 2300.0593308359385 dt: 0:00:37.714316\n",
      "epoch complete: 262 cost: 2280.0888693258166 dt: 0:00:37.623296\n",
      "epoch complete: 263 cost: 2290.9603508487344 dt: 0:00:37.661149\n",
      "epoch complete: 264 cost: 2279.973698347807 dt: 0:00:37.984112\n",
      "epoch complete: 265 cost: 2257.941972836852 dt: 0:00:37.367897\n",
      "epoch complete: 266 cost: 2267.044733375311 dt: 0:00:37.558587\n",
      "epoch complete: 267 cost: 2256.5365036576986 dt: 0:00:37.569628\n",
      "epoch complete: 268 cost: 2263.913817048073 dt: 0:00:37.588695\n",
      "epoch complete: 269 cost: 2273.5321155786514 dt: 0:00:37.485108\n",
      "epoch complete: 270 cost: 2268.033263593912 dt: 0:00:37.728895\n",
      "epoch complete: 271 cost: 2252.3398798704147 dt: 0:47:25.894019\n",
      "epoch complete: 272 cost: 2263.982966452837 dt: 0:00:38.913352\n",
      "epoch complete: 273 cost: 2267.867979131639 dt: 0:00:38.225117\n",
      "epoch complete: 274 cost: 2259.5423522740602 dt: 0:00:37.836503\n",
      "epoch complete: 275 cost: 2259.716867595911 dt: 0:00:37.652741\n",
      "epoch complete: 276 cost: 2272.4145880043507 dt: 0:00:37.970601\n",
      "epoch complete: 277 cost: 2272.2998067885637 dt: 0:00:37.989481\n",
      "epoch complete: 278 cost: 2243.211842805147 dt: 0:00:38.112405\n",
      "epoch complete: 279 cost: 2264.5546577870846 dt: 0:00:37.642887\n",
      "epoch complete: 280 cost: 2262.9585615098476 dt: 0:00:37.632381\n",
      "epoch complete: 281 cost: 2247.0480865761638 dt: 0:00:37.503978\n",
      "epoch complete: 282 cost: 2263.3400411233306 dt: 0:00:37.562523\n",
      "epoch complete: 283 cost: 2251.677269399166 dt: 0:00:37.680377\n",
      "epoch complete: 284 cost: 2251.8348082005978 dt: 0:00:37.503775\n",
      "epoch complete: 285 cost: 2248.3398562148213 dt: 0:00:38.046193\n",
      "epoch complete: 286 cost: 2250.4336644262075 dt: 0:00:37.892866\n",
      "epoch complete: 287 cost: 2265.625036895275 dt: 0:00:37.888753\n",
      "epoch complete: 288 cost: 2259.751643270254 dt: 0:00:37.664752\n",
      "epoch complete: 289 cost: 2265.95613694191 dt: 0:00:37.939690\n",
      "epoch complete: 290 cost: 2247.9134909547865 dt: 0:00:37.860201\n",
      "epoch complete: 291 cost: 2256.6410765349865 dt: 0:00:37.881136\n",
      "epoch complete: 292 cost: 2249.9632958993316 dt: 0:00:37.935891\n",
      "epoch complete: 293 cost: 2253.3009689301252 dt: 0:00:37.867532\n",
      "epoch complete: 294 cost: 2248.281450152397 dt: 0:00:37.823354\n",
      "epoch complete: 295 cost: 2258.925968900323 dt: 0:00:38.043973\n",
      "epoch complete: 296 cost: 2234.5788401067257 dt: 0:00:37.780868\n",
      "epoch complete: 297 cost: 2247.8698789924383 dt: 0:00:37.846617\n",
      "epoch complete: 298 cost: 2258.902218133211 dt: 0:00:38.211166\n",
      "epoch complete: 299 cost: 2226.0451618283987 dt: 0:00:37.829540\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "directory = logdir\n",
    "#get the data\n",
    "paths, title2indx = get_data()\n",
    "    \n",
    "# number fo different titles\n",
    "number_of_titles = len(title2indx)\n",
    "\n",
    "# configuration parameters for skipgram\n",
    "context_size = 3\n",
    "learning_rate = 0.025\n",
    "final_learning_rate = 0.0001\n",
    "num_negatives = 5 #number of negative sampling titles\n",
    "epochs = 300;\n",
    "D = 50 # word embedding size, as in NxD\n",
    "    \n",
    "#learning rate decay\n",
    "learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "#distribution for drawing negative samples\n",
    "p_neg = get_negative_sampling_distribution(paths)\n",
    "    \n",
    "# Parameters\n",
    "W = np.random.randn(number_of_titles, D).astype(np.float32) # input-to-hidden\n",
    "V = np.random.randn(D, number_of_titles).astype(np.float32) #hidden-to-output\n",
    "    \n",
    "#create the model\n",
    "with tf.name_scope(\"Title2Vec_place_holders\"):\n",
    "    tf_input = tf.placeholder(tf.int32, shape=(None,),name=\"Input\")\n",
    "    tf_negtitle = tf.placeholder(tf.int32, shape=(None,), name=\"Negative_Title\")\n",
    "    tf_context = tf.placeholder(tf.int32, shape=(None,), name=\"Targets/context\") #targets (context)\n",
    "tfW = tf.Variable(W,name=\"W\")\n",
    "tfV = tf.Variable(V.T, name=\"V_transposed\")\n",
    "    \n",
    "def dot(A, B):\n",
    "    C = A*B\n",
    "    return tf.reduce_sum(C, axis=1)\n",
    "    \n",
    "#correct middle title output\n",
    "emb_input = tf.nn.embedding_lookup(tfW, tf_input) # 1xD\n",
    "emb_output = tf.nn.embedding_lookup(tfV, tf_context) #NxD\n",
    "correct_output = dot(emb_input, emb_output) # N\n",
    "    \n",
    "pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones(tf.shape(correct_output)), logits=correct_output)\n",
    "    \n",
    "# incorrect middle title output. incorrect middle title comes from negative sampling\n",
    "emb_input = tf.nn.embedding_lookup(tfW, tf_negtitle)\n",
    "incorrect_output = dot(emb_input, emb_output)\n",
    "    \n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros(tf.shape(incorrect_output)), logits=incorrect_output)\n",
    "    \n",
    "#total loss\n",
    "loss = tf.reduce_mean(pos_loss) + tf.reduce_mean(neg_loss)\n",
    "    \n",
    "# Optimizer\n",
    "train_op = tf.train.MomentumOptimizer(0.1, momentum=0.9).minimize(loss)\n",
    "    \n",
    "#make tensor flow session\n",
    "sess = tf.InteractiveSession() #interactive so i can keep it on more cells\n",
    "init_op = tf.global_variables_initializer()\n",
    "    \n",
    "#set up tensorboard\n",
    "loss_summary = tf.summary.scalar('Loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "sess.run(init_op)\n",
    "    \n",
    "# A list to save the cost to be plotted later\n",
    "#.   This will eventually be replced by tensor board stuff\n",
    "costs = []\n",
    "    \n",
    "# number of total titles in corpus\n",
    "total_titles = sum(len(path) for path in paths)\n",
    "print(\"Total number of titles:\", total_titles)\n",
    "  \n",
    "# for subsampling each sentence\n",
    "threshold = 1e-5\n",
    "p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "    \n",
    "#train the model\n",
    "for epoch in range(epochs):\n",
    "    #randomize what order we see the paths\n",
    "    np.random.shuffle(paths)\n",
    "        \n",
    "    #accumulate the cost\n",
    "    cost = 0;\n",
    "    counter = 0;\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    negtitles = []\n",
    "    t0 = datetime.now()\n",
    "    for path in paths:\n",
    "        #keep only some of the titles based on p_neg, this is for speeding up the training. We may need to play around with this to see if it is really helping.\n",
    "        path = [t for t in path if np.random.random() < (1 - p_drop[t])]\n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "            \n",
    "        #pick randomly which order to see the middle titles so we don't always see samples in the same order\n",
    "        randomly_ordered_positions = np.random.choice(len(path),\n",
    "                                                         size=len(path),\n",
    "                                                         replace=False)\n",
    "            \n",
    "        for j, pos in enumerate(randomly_ordered_positions):\n",
    "            # title is the \"input title\" that we will find the context titles around.\n",
    "            title = path[pos]\n",
    "                \n",
    "            #get the positive context titles/negative samples\n",
    "            context_titles = get_context(pos, path, context_size)\n",
    "            neg_title = np.random.choice(number_of_titles, p=p_neg) #this is a single negative middle word\n",
    "            #the negative middle word is put with the context words for the negative samples\n",
    "                \n",
    "            n = len(context_titles)\n",
    "            inputs += [title]*n\n",
    "            negtitles += [neg_title]*n\n",
    "            targets += context_titles\n",
    "\n",
    "        if len(inputs) >= 64:    #batch size of 64\n",
    "                \n",
    "                # Run one iteration of the network.                 \n",
    "            _, c = sess.run((train_op, loss),\n",
    "                            feed_dict={tf_input: inputs,\n",
    "                                       tf_negtitle: negtitles,\n",
    "                                       tf_context: targets})\n",
    "            cost += c\n",
    "              \n",
    "            #reset the batch\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            negtitles = []\n",
    "                    \n",
    "        counter += 1\n",
    "#         if counter % 100 == 0:\n",
    "#                 #may want to replace this with tensorboard stuff\n",
    "#             print(\"processed %s / %s\\r\" % (counter, len(paths)))    \n",
    "                \n",
    "    # print stuff so we know things are working\n",
    "    dt = datetime.now() - t0\n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "            \n",
    "    #save the cost\n",
    "    costs.append(cost)\n",
    "        \n",
    "    #tensor board update\n",
    "    summary_str = loss_summary.eval(session=sess, feed_dict={loss:cost})\n",
    "    file_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "     #update the learning rate\n",
    "    learning_rate -= learning_rate_delta\n",
    "            \n",
    "#plot the cost per iteration\n",
    "plot.plot(range(len(costs)), costs)\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.ylabel(\"Cost\")\n",
    "plot.show()\n",
    "        \n",
    "#get parameters\n",
    "W, VT = sess.run((tfW, tfV))\n",
    "V = VT.T   \n",
    "\n",
    "#save the model\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "            \n",
    "with open('%s/title2indx.json' % directory, 'w') as f:\n",
    "    json.dump(title2indx, f)\n",
    "            \n",
    "np.savez('%s/weights.npz' % directory, W, V)\n",
    "        \n",
    "# #return the model\n",
    "# return title2indx, W, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard embedding visualization\n",
    "The cells below are my attempt to get the 3d representation of the title embeddings in tensorboard. I have not been very successful in this attempt, so the cells below don't have to be run. Also note that at the time of writing this, the things that do show up in tensorboard are labeled incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_shape = (tf.cast(W.shape[0], tf.int32), tf.cast(W.shape[1],tf.int32))\n",
    "embedding_var = tf.Variable(tf.zeros(embedding_shape ), \n",
    "                            name='W_embedding')\n",
    "# assign the tensor that we want to visualize to the embedding variable\n",
    "embedding_assign = embedding_var.assign(tfW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Create a config object to write the configuration parameters\n",
    "config = projector.ProjectorConfig()\n",
    "config.model_checkpoint_path = os.path.join(directory, \"model.ckpt\")\n",
    "\n",
    "# Add embedding variable\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# # Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n",
    "# embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "# # Specify where you find the sprite. -> we will create this image later\n",
    "# embedding.sprite.image_path = 'sprite_images.png'\n",
    "# embedding.sprite.single_image_dim.extend([img_w, img_h])\n",
    "\n",
    "# Write a projector_config.pbtxt in the logs_path.\n",
    "# TensorBoard will read this file during startup.\n",
    "projector.visualize_embeddings(file_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n",
      "(8000, 50)\n"
     ]
    }
   ],
   "source": [
    "# a cell I have been using for debugging\n",
    "print(tf_context.shape)\n",
    "type(W)\n",
    "print(W.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorFlow_logs/run-20181119_1833/model.ckpt-300'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run session to evaluate the tensor\n",
    "w_emb = sess.run(embedding_assign, feed_dict={tf_context:targets})\n",
    "\n",
    "# Save the tensor in model.ckpt file\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(directory, \"model.ckpt\"), epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where I run the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths/title2indx.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6fb263096b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f9b3690aacb5>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/title2indx.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtitle2indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnpz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/weights.npz'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths/title2indx.json'"
     ]
    }
   ],
   "source": [
    "directory = \"/Users/gregorycolledge/gcolledge/gcolledge/MSD_6019/Notebooks/tensorFlow_logs/run-20181101_allPaths\"\n",
    "\n",
    "t, w, v = load_model(directory)\n",
    "test_model(t, w, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1157f729c6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intern'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(w))\n",
    "print(w.shape)\n",
    "print(w[t['intern']])\n",
    "\n",
    "print(type(v))\n",
    "print(v.shape)\n",
    "print(v[t['intern']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the session!\n",
    "This needs to be run after the training cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test for my github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
